{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.config import ROOT_DIR\n",
    "from rag.config import EMBEDDING_DIMENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Retrieval\n",
    "\n",
    "With embedded chunks now stored in the vector database, we can perform the retrieval of embedded chunks based on input query. We'll start by using the same embedding model that we used to embed our documents to embed the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "\n",
    "#Embed query\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "query = \"What is the default size for map_batches?\"\n",
    "embedding_query = np.array(embedding_model.embed_query(query))\n",
    "print(len(embedding_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can retrieve the top-k relevant chunks by extracting the closest embedded chunks to our embedded query. We use cosine distance, which is a technique to search for similarities between two vectors to search for closest chunks. There are many other options to select top most chunks. Once we retrieve the `num_chunks`, we can collect the text from each chunk and use it as context to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "# Get context text\n",
    "num_chunks = 5\n",
    "with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "    register_vector(conn)\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT *, (embedding <=> %s) AS similarity_score FROM document ORDER BY similarity_score LIMIT %s\", (embedding_query, num_chunks))\n",
    "        rows = cur.fetchall()\n",
    "        ids = [row[0] for row in rows]\n",
    "        context = [{\"text\": row[1]} for row in rows]\n",
    "        sources = [row[2] for row in rows]\n",
    "        scores = [row[4] for row in rows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19585\n",
      "0.06648371622447513\n",
      "https://docs.ray.io/en/master/data/api/doc/ray.data.Dataset.map_batches.html#ray-data-dataset-map-batches\n",
      "The actual size of the batch provided to fn may be smaller than\n",
      "batch_size if batch_size doesn’t evenly divide the block(s) sent\n",
      "to a given map task. Default batch_size is 1024 with “default”.\n",
      "compute – This argument is deprecated. Use concurrency argument.\n",
      "\n",
      "25430\n",
      "0.07046984786153998\n",
      "https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-size\n",
      "batch_size.\n",
      "Note\n",
      "The default batch size depends on your resource type. If you’re using CPUs,\n",
      "the default batch size is 4096. If you’re using GPUs, you must specify an explicit\n",
      "batch size.\n",
      "\n",
      "25631\n",
      "0.07540523106988828\n",
      "https://docs.ray.io/en/master/data/batch_inference.html#configuring-batch-size\n",
      "# Specify that each input batch should be of size 2.\n",
      "ds.map_batches(assert_batch, batch_size=2)\n",
      "Caution\n",
      "The default batch_size of 4096 may be too large for datasets with large rows\n",
      "(for example, tables with many columns or a collection of large images).\n",
      "\n",
      "25626\n",
      "0.08636225814142995\n",
      "https://docs.ray.io/en/master/data/batch_inference.html#configuring-batch-size\n",
      "Configuring Batch Size#\n",
      "Configure the size of the input batch that’s passed to __call__ by setting the batch_size argument for ds.map_batches()\n",
      "\n",
      "26430\n",
      "0.09422267950873586\n",
      "https://docs.ray.io/en/master/tune/getting-started.html#setting-up-a-tuner-for-a-training-run-with-tune\n",
      "batch_size=64,\n",
      "        shuffle=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(context):\n",
    "    print (ids[i])\n",
    "    print (scores[i])\n",
    "    print (sources[i])\n",
    "    print (item[\"text\"])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap the entire code into a function for easier convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, embedding_model, num_chunks=5):\n",
    "    embedding_query = np.array(embedding_model.embed_query(query))\n",
    "    with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "        register_vector(conn)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT *, (embedding <=> %s) AS similarity_score FROM document ORDER BY similarity_score LIMIT %s\", (embedding_query, num_chunks))\n",
    "            rows = cur.fetchall()\n",
    "            semantic_context = [{\"id\": row[0], \"text\": row[1], \"source\": row[2], \"score\": row[4]} for row in rows]\n",
    "\n",
    "    return semantic_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the context to generate a response from our LLM. Without this relevant context that we retrieved, the LLM may not have been able to accurately answer our question. And as our data grows, we can just as easily embed and index any new data and be able to retrieve it to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.utils import get_client\n",
    "from rag.generate import prepare_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "        llm, temperature=0.0, stream=True,\n",
    "        system_content=\"\", assistant_content=\"\", user_content=\"\",\n",
    "        max_retries=1, retry_interval=60):\n",
    "    \"\"\"Generate an response from LLM\"\"\"\n",
    "    retry_count = 0\n",
    "    client = get_client(llm)\n",
    "    messages = [{\"role\": role, \"content\": content } for role, content in [(\"system\", system_content), (\"assistant\", assistant_content), (\"user\", user_content)] if content]\n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=llm,\n",
    "                messages=messages,\n",
    "                stream=stream,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            return prepare_response(chat_completion, stream=stream)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occured: {e}\")\n",
    "            time.sleep(retry_interval)\n",
    "            retry_count += 1\n",
    "\n",
    "    return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We’re using a temperature of 0.0 to enable reproducible experiments but you should adjust this based on your use case. For use cases that need to always be factually grounded, we recommend very low temperature values while more creative tasks can benefit from higher temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The actual size of the batch provided to fn may be smaller than\\nbatch_size if batch_size doesn’t evenly divide the block(s) sent\\nto a given map task. Default batch_size is 1024 with “default”.\\ncompute – This argument is deprecated. Use concurrency argument.', 'batch_size.\\nNote\\nThe default batch size depends on your resource type. If you’re using CPUs,\\nthe default batch size is 4096. If you’re using GPUs, you must specify an explicit\\nbatch size.', '# Specify that each input batch should be of size 2.\\nds.map_batches(assert_batch, batch_size=2)\\nCaution\\nThe default batch_size of 4096 may be too large for datasets with large rows\\n(for example, tables with many columns or a collection of large images).', 'Configuring Batch Size#\\nConfigure the size of the input batch that’s passed to __call__ by setting the batch_size argument for ds.map_batches()', 'batch_size=64,\\n        shuffle=True)']\n"
     ]
    }
   ],
   "source": [
    "context_results = semantic_search(query=query, embedding_model=embedding_model, num_chunks=5)\n",
    "context = [item[\"text\"] for item in context_results]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.OpenAI object at 0x29a6cbad0>\n",
      "Exception occured: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Exception occured: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "query = \"What is the default batch size for map_batches?\"\n",
    "response = generate_response(\n",
    "    # llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    llm=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    stream=True,\n",
    "    system_content=\"Answer the query using the context provided. Be succinct.\",\n",
    "    user_content=f\"query: {query}, context: {context}\")\n",
    "# Stream response\n",
    "for content in response:\n",
    "    print(content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine context retrieval and answer response generation into a conventient query agent that we can easily use to generate our responses. This Conversational query agent will take care of setting up our agent (embedding model and LLM agent), as well as the context retrieval part, and pass it to the LLM agent for response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.embed import get_embedding_model\n",
    "from rag.utils import get_num_tokens, trim\n",
    "from rag.config import MAX_CONTEXT_LENGTHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class QueryAgent:\n",
    "    def __init__(self, embedding_model_name=\"thenlper/gte-base\",\n",
    "                 llm=\"gpt-3.5-turbo\", temperature=0.0, \n",
    "                 max_content_length=4096, system_content=\"\", assistant_content=\"\") -> None:\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name=embedding_model_name,\n",
    "            model_kwargs={\"device\":\"cpu\"},\n",
    "            encode_kwargs={\"device\": \"cpu\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "        # Context length (restrict input length to 50% of total context length)\n",
    "        max_context_length = int(0.5*max_context_length)\n",
    "\n",
    "        #LLM\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.context_length = max_context_length - get_num_tokens(system_content + assistant_content)\n",
    "        self.system_content = system_content\n",
    "        self.assistant_content = assistant_content\n",
    "\n",
    "\n",
    "    def __call__(self, query, num_chunks=5, stream=True) -> Any:\n",
    "        # Get Sources and Context\n",
    "        context_results = semantic_search(\n",
    "            query=query,\n",
    "            embedding_model=self.embedding_model,\n",
    "            num_chunks=num_chunks\n",
    "        )\n",
    "\n",
    "        #Generate Response\n",
    "        context = [item[\"text\"] for item in context_results]\n",
    "        sources = [item[\"source\"] for item in context_results]\n",
    "        user_content = f\"query: {query}, context: {context}\"\n",
    "\n",
    "        answer = response = generate_response(\n",
    "            llm=self.llm,\n",
    "            temperature=self.temperature,\n",
    "            stream=stream,\n",
    "            system_content=self.system_content,\n",
    "            user_content=trim(user_content, self.context_length)\n",
    "        )\n",
    "\n",
    "        #Result\n",
    "        result  =  {\n",
    "            \"question\": query,\n",
    "            \"sources\": sources,\n",
    "            \"answer\": answer,\n",
    "            \"llm\": self.llm,\n",
    "        }\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can use our RAG application in just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "llm = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the default batch size for map_batches?\"\n",
    "system_content = \"Answer the query using the context provided. Be succinct.\"\n",
    "agent = QueryAgent(\n",
    "    embedding_model_name=embedding_model_name,\n",
    "    llm=llm,\n",
    "    max_context_length=MAX_CONTEXT_LENGTHS[llm],\n",
    "    system_content=system_content)\n",
    "result = agent(query=query, stream=False)\n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
